{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1a8e17",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee8960a",
   "metadata": {},
   "source": [
    "* [takeaway].<br>\n",
    "\n",
    "* [takeaway]. <br>\n",
    "\n",
    "* [takeaway]. <br>\n",
    "\n",
    "* [takeaway]. <br>\n",
    "\n",
    "* [takeaway]. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75874666-e3ac-4cc7-ac24-b38a97ef5f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:36:41.372727Z",
     "iopub.status.busy": "2023-02-05T21:36:41.372173Z",
     "iopub.status.idle": "2023-02-05T21:36:41.379077Z",
     "shell.execute_reply": "2023-02-05T21:36:41.377483Z",
     "shell.execute_reply.started": "2023-02-05T21:36:41.372688Z"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386c1731-7a29-43a4-8baf-71c9db62a787",
   "metadata": {},
   "source": [
    "In this post, I train Karpathy's __[nanoGPT](https://johncollinsai-nanogpt-voqqf4ls3a-as.a.run.app/)__ on high-frequency (tick-by-tick) data for __[AAPL](https://www.google.com/search?q=aapl&oq=AAPL&aqs=chrome.0.0i512l5j69i61l3.1590j1j9&sourceid=chrome&ie=UTF-8)__ and __[JPM](https://www.google.com/search?q=jpm+stock+price&oq=JPM+stock+pri&aqs=chrome.0.0i512j69i57j0i512l8.4577j1j9&sourceid=chrome&ie=UTF-8)__. I want to see how nanoGPT performs as a volatility predictor.  I also want to explore the use of LLMs for tasks, in this case volatility prediction, that are typically performed by models more specific to finance.  In the case of volatility prediction, the established model classes include stochastic volatility models such as the __[MSM](https://github.com/johncollinsai/markov-switching-multifractal)__ of Calvet & Fisher, ARCH and GARCH, and Jump Diffusion models. More recently deep learning has been applied to volatility prediction and this __[post](https://johncollinsai-deep-learning-finance-voqqf4ls3a-as.a.run.app/)__ describes these developments in some detail. However, the application of LLMs to volatility prediction appears to be quite novel and the use of nanoGPT provides a great basis for an under-the-hood examination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d218c",
   "metadata": {},
   "source": [
    "**High-frequency data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a4996",
   "metadata": {},
   "source": [
    "See my earlier __[post](https://johncollinsai-high-frequency-data-voqqf4ls3a-as.a.run.app/)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673a1c4",
   "metadata": {},
   "source": [
    "**NanoGPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a732d-b988-4548-b377-640ff39eb629",
   "metadata": {},
   "source": [
    "See my earlier __[post](https://johncollinsai-nanogpt-voqqf4ls3a-as.a.run.app/)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec0a89",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e305541",
   "metadata": {},
   "source": [
    "I begin with __[my earlier implementation of Karpathy's nanoGPT](https://github.com/johncollinsai/nanogpt)__. \n",
    "\n",
    "REPLACE THIS: Starting with a very simple bigram language model, following Karpathy, I define and build a transformer piece by piece.  \n",
    "\n",
    "REPLACE THIS: Then I train it on a text dataset.  \n",
    "\n",
    "I use an NVIDIA GeForce RTX 3080 Ti Laptop GPU and a deep learning framework that includes PyTorch, CUDA, cuDNN, and NVIDIA Drivers, on Ubuntu 22.04 LTS.  Source code as always may be found on __[my GitHub](https://github.com/johncollinsai/nanogpt)__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896def4a-31c8-4aa2-805e-4ecd5231a29c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-05T21:37:21.573542Z",
     "iopub.status.busy": "2023-02-05T21:37:21.572887Z",
     "iopub.status.idle": "2023-02-05T21:37:21.577911Z",
     "shell.execute_reply": "2023-02-05T21:37:21.576606Z",
     "shell.execute_reply.started": "2023-02-05T21:37:21.573523Z"
    }
   },
   "source": [
    "# Building and training a GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b1fc-b610-44c1-885d-1935aee8207e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline (bigram) language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfd5cb",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4857982f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Device name: NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# check GPU (if working on local machine)\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"CUDA is not available.\")\n",
    "    print(f\"device: {device}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6c82752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for running in docker image\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416923fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_113435/2357321924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of dataseet in characters: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input.txt'"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"length of dataseet in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a44601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# show unique characters appearing in the dataset (note the space character, which is first in the set): i.e., the vocabulary of possible characters the model can see or emit\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c21805",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "> Build a simple encoder and  decoder: i.e., take a string, output a list of integers, where each character is a token. The approach below is similar to, but much more simplified than: __[goolge sentencepiece](https://github.com/google/sentencepiece)__ (which uses sub-word encodings) and __[OpenAI tiktoken](https://github.com/openai/tiktoken)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bc426f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 53, 42, 39, 63, 1, 47, 57, 1, 44, 56, 47, 42, 39, 63, 6, 1, 50, 53, 53, 49, 47, 52, 45, 1, 44, 53, 56, 61, 39, 56, 42, 1, 58, 53, 1, 58, 46, 43, 1, 61, 43, 43, 49, 43, 52, 42, 2]\n",
      "today is friday, looking forward to the weekend!\n"
     ]
    }
   ],
   "source": [
    "# convert the raw text as a string into some sequence of integers according to some vocabulary of possible elements\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# build a simple encoder and decoder, effectively a tokenizer and detokenizer\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"today is friday, looking forward to the weekend!\"))\n",
    "print(decode(encode(\"today is friday, looking forward to the weekend!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc914948",
   "metadata": {},
   "source": [
    "> now I have a tokenizer and detokenizer, I can convert the raw text into a sequence of integers, i.e., I can tokenize the entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2edc8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "# encode training dataset and store it in a torch.tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c31f70-00ec-4a57-b6b9-c610454e7f39",
   "metadata": {},
   "source": [
    "#### Train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff404893-dff5-4e84-9e12-92f11cfc34b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 90:10 train:val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8320008e-4209-4004-b6ed-6c2cc9b3eb4c",
   "metadata": {},
   "source": [
    "#### Loading the data\n",
    "> I set the time dimension (i.e., the contexts) of the tensors feeding into the transformer equal to a maximum of 8 characters (i.e., I set block_size = 8).  Note: I train on block_size+1 because the transformer trains on the first 8 characters and predicts the +1th or 9th character.  Put another way, the transformer sees contexts from one character thru block_size. <br>\n",
    "\n",
    "> And I set the batch dimension of the tensors feeding into the transformer to 4, so batch_size = 4 (i.e., 4 independent sequences will be processed in parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26380bac-baa0-47a7-b19a-536b6b0f1caf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set block_size = 8 to train on []:block_size+1] = 8+1 characters at a time\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfdae44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Illustrating how the transformer trains on the first 8 characters and predicts the +1th or 9th character:\n",
      "when input is tensor([18]), the target is 47\n",
      "when input is tensor([18, 47]), the target is 56\n",
      "when input is tensor([18, 47, 56]), the target is 57\n",
      "when input is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "# +1 because we want to predict the next character, thus block_size+1 allows us to do that, i.e., the transformer trains on the first 8 characters and predicts the +1th or 9th character\n",
    "# to illustrate:\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "print('Illustrating how the transformer trains on the first 8 characters and predicts the +1th or 9th character:')\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context}, the target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca53410",
   "metadata": {},
   "source": [
    "#### A quick note on random seed selection\n",
    "\n",
    "> In an interesting __[paper](https://arxiv.org/abs/2109.08203)__ David Picard investigates the effect of random seed selection on accuracy when using deep learning architectures for computer vision and posits that Torch.manual_seed(3407) is all you need!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb66009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the tensor input to the transformer: \n",
      " tensor([[32, 39, 49, 43,  1, 58, 46, 53],\n",
      "        [59, 56,  1, 54, 56, 47, 52, 41],\n",
      "        [57, 53, 51, 43,  1, 51, 43, 56],\n",
      "        [57,  1, 58, 56, 59, 43,  2,  0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# I set the batch dimension of the tensors feeding into the transformer to 4, so batch_size = 4 (i.e., 4 independent sequences will be processed in parallel).\n",
    "torch.manual_seed(3407)\n",
    "batch_size = 4\n",
    "block_size = 8  \n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # move data to GPU\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print('Here is the tensor input to the transformer:',\n",
    "      '\\n', \n",
    "      xb      \n",
    "      )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a9b6b-3710-4454-a648-2daef588b99f",
   "metadata": {},
   "source": [
    "#### baseline (bigram) language model\n",
    "\n",
    "Following __[Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)__, I implement a very simple neural network, the bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bec0de5-afc3-4ada-8c24-ef2e5325d7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([32, 65])\n",
      "loss: tensor(4.4231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pTFwSpp,f.v-;LR-;DA,O:rGMbv3OqDlpuo-SxIMtqCPawLaD;iC O'-N$sr?,y;Dgx&uJvha?qU.RXFqe!3CLnq,ZAcdW-dxvq\n",
      "ijb-dmxN-lLtI'UsNajeE3gH??!m3zz:nMgrVgHyRJd;MVWy'nEDSCT!QA;myMPVPLnvyjMWXFw,LweP,WSzdPrvcWXecNIcLtcPrPbGIzVH.nqckUK;XfAco',QFJ3'T !a-$Nemy,WmkUIx?mO!sJwEywCCk,W:Jv3V&PjhvEooQF3taT\n",
      "3&u!XCikXcY\n",
      "?xIzQrGW\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(3407)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C, i.e., a batch by time (context) by channel tensor, where channel is vocab size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reorganize logits tensor from (B, T, C) to (B*T, C) in order to fit pytorch's cross_entropy loss function\n",
    "            B, T, C = logits.shape \n",
    "            logits = logits.view(B*T, C) \n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # cross_entropy here computes negative log likelihood loss\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get the probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device) # move model to GPU\n",
    "logits, loss = m(xb, yb)\n",
    "print('logits shape:', logits.shape)\n",
    "print('loss:', loss)\n",
    "\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device), here created on-the-fly by print() on the GPU\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=300)[0].tolist())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2830757",
   "metadata": {},
   "source": [
    "> The model is untrained and provides predictions that are random, so the output is meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3b698-2682-4dfc-9ff1-8a420d51e037",
   "metadata": {},
   "source": [
    "#### Training the bigram model\n",
    "\n",
    "> I now train the bigram model to make it less random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c539ac77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0b42ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.5604467391967773\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # increase the batch size from 4 to 32 to speed up training\n",
    "for steps in range(10000): # increase the number of steps to train for, to improve results\n",
    "\n",
    "    # get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print('loss:', loss.item()) # training for 10000 steps brings the loss down to ~2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c16952a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Afre an nthe t y.\n",
      "I twl d, h bstave: anr h? towll\n",
      "BUStil ilouniasthechyord IF shaty vouby, m aysie fon Malld aty acoghas; histhofok ang titr. may, o mar, we gel adadico y mereengoowe.\n",
      "Hiplouplloproousseathes l we f Jater, thee,\n",
      "\n",
      "Hwncoshy momyow, r agh afurst thes hendee: byoon t MIILELIZMend, cuthe\n"
     ]
    }
   ],
   "source": [
    "# As above, context = torch.zeros((1,1), dtype=torch.long, device=device), here created on-the-fly by print() on the GPU\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12526d05",
   "metadata": {},
   "source": [
    "> The model is making progress.  But it's still a very simple model and the tokens are not yet talking to each other.  It's predictions show a somewhat better language-like structure, but are still random, and the output meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d1a718-bb5f-4479-a576-465adde0082c",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "\n",
    "> I now write the first self-attention block for processing the tokens, following several steps, each progressively more effective, that hopefully help to make the self-attention contstruct clearer. <br>\n",
    "\n",
    "> Let's start with a very simple example, which essentially relates tokens to each other via their history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b3efd10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example\n",
    "torch.manual_seed(3407)\n",
    "B,T,C = 4,8,2 # batch size, time steps, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65089f35-33e4-45de-821c-e56ac1f5863e",
   "metadata": {},
   "source": [
    "#### Averaging past context with for loops (weakest form of aggregation)\n",
    "\n",
    "> A simple way to enable tokens to communicate in the manner we desire (i.e., with the tokens that precede them in T), is to calculate an average of all the preceding elements. Consider, for example, the fifth token: take the channels that make up that information at that step, but also the channels from the fourth step, third step, second and first steps, and average them.  This creates, effectively, a feature vector that summarizes the 5th token in the context of its history.  An average like this is an extremely weak and lossy, i.e., a lot of information about the spacial arrangements of the tokens is lost. <br>\n",
    "\n",
    "> So, for every batch element independently, for every $n^{th}$ token in that sequence, calculate the average of all the vectors in all the previous tokens and also at the $n^{th}$ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb497036-c809-44fc-8152-331cb6594706",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1703, -0.8613],\n",
      "        [-0.6225,  1.0247],\n",
      "        [ 0.3506,  0.8032],\n",
      "        [ 0.0865, -0.9623],\n",
      "        [-1.6784,  1.3681],\n",
      "        [-0.1882,  1.7510],\n",
      "        [ 0.5818, -0.3983],\n",
      "        [ 1.4324, -0.6142]])\n",
      "xbow averages everything up to the current location of the nth token:  \n",
      " tensor([[ 0.1703, -0.8613],\n",
      "        [-0.2261,  0.0817],\n",
      "        [-0.0339,  0.3222],\n",
      "        [-0.0038,  0.0011],\n",
      "        [-0.3387,  0.2745],\n",
      "        [-0.3136,  0.5206],\n",
      "        [-0.1857,  0.3893],\n",
      "        [ 0.0166,  0.2639]])\n"
     ]
    }
   ],
   "source": [
    "# I want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C)) # bow for bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t, C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print('xbow averages everything up to the current location of the nth token: ', '\\n',\n",
    "      xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8b09bd-e7b5-4de1-985f-8423193eb8ac",
   "metadata": {},
   "source": [
    "#### Self-attention: matrix multiply as weighted aggregation\n",
    "\n",
    "> Karpathy shows how to use matrix multiplication to increase the efficiency of the above operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9ba9f42-1d4b-4aba-bc74-8d8a58f69272",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones((T,T))) # wei denotes weights, torch.tril provides lower triangular matrix\n",
    "wei = wei / wei.sum(1, keepdim=True) # normalize weights so that they sum to 1\n",
    "xbow2 = wei @ x #  (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow, xbow2) # check that the two methods give the same result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b513a4-5e38-46b7-9d4c-eb7fd43ce2d9",
   "metadata": {},
   "source": [
    "#### Adding softmax\n",
    "\n",
    "> Applying a softmax to each row to normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8783fa4f-c174-4a88-80cb-be8738e08ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones((T,T))) # tril matrix of lower triangular ones\n",
    "wei = torch.zeros((T,T)) # wei begins as a matrix of zeros\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # weights for the future tokens are set to -inf, so future tokens are ignored\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a01da-694c-4159-85ca-b49135fd23ef",
   "metadata": {},
   "source": [
    "#### Self-attention\n",
    "\n",
    "> Now observe a single head perform self attention. A \"head\" refers to what is effectively a sub-network that processes input sequences independently. In transformers, self-attention normally comprises multiple attention heads that allows the model to attend to different parts of the input sequence at different levels of granularity, enabling the model to capture more diverse and nuanced relationships between the different elements of the input.  Thus, self-attention enables the model to gather information from the past and apply it in a data-dependent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f54128-e5c2-467e-bd81-2a40327dc60a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(3407)\n",
    "B, T, C = 4, 8, 32 # batch, time, channels (recall, channels is dimensionality of the input, e.g., now 32 for a 32-dimensional embedding)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Observe a single head perform self-attention\n",
    "head_size = 16 # the head hyperparameter, being the number of dimensions in the query, key, and value vectors\n",
    "key = nn.Linear(C, head_size, bias=False) # key vector roughly speaking means what do I contain\n",
    "query = nn.Linear(C, head_size, bias=False) # query vector roughly speaking means what am I looking for\n",
    "value = nn.Linear(C, head_size, bias=False) # value vector roughly speaking means what do I return\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "# the affinities are obtained by taking the dot product of the query and key vectors\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, head_size) @ (B, head_size, T) --> (B, T, T) --> wei is roughly speaking the affinity matrix\n",
    "\n",
    "tril = torch.tril(torch.ones((T,T))) \n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "v = value(x) # results in 16-dimensional vectors because that is the head_size\n",
    "out = wei @ v \n",
    "\n",
    "out.shape # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822fd86",
   "metadata": {},
   "source": [
    "> Observe $wei$, the matrix of affinities, as a matrix of lower triangular values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a87d0047",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9217, 0.0783, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2666, 0.1544, 0.5789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0332, 0.4348, 0.2287, 0.3034, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0853, 0.0492, 0.1398, 0.5914, 0.1343, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0553, 0.2033, 0.0449, 0.5934, 0.0320, 0.0711, 0.0000, 0.0000],\n",
       "        [0.1148, 0.0771, 0.0900, 0.0522, 0.0507, 0.2886, 0.3266, 0.0000],\n",
       "        [0.1356, 0.0336, 0.0196, 0.0464, 0.0245, 0.2620, 0.2610, 0.2173]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f818f9-4516-4c3f-9b8f-2d8de4beb9b1",
   "metadata": {},
   "source": [
    "## Building the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f07d79-0203-4558-93a0-5e8974b317ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:03:37.171153Z",
     "iopub.status.busy": "2023-02-09T04:03:37.170812Z",
     "iopub.status.idle": "2023-02-09T04:03:37.176160Z",
     "shell.execute_reply": "2023-02-09T04:03:37.174873Z",
     "shell.execute_reply.started": "2023-02-09T04:03:37.171132Z"
    }
   },
   "source": [
    "#### Inserting a single self-attention block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6081db1b-3530-4863-aff1-ee0d33f7f24f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single self-attention block\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097653db-0fd6-4d50-ae8e-31188085e453",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Updating the bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ecd059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5923,  2.3207, -2.0654,  ..., -3.3131, -0.5611, -2.9758],\n",
      "        [-0.7643,  2.3739, -1.8800,  ..., -5.2282,  0.1946, -5.0154],\n",
      "        [-0.7329,  2.2207, -1.6798,  ..., -4.9584, -0.7605, -4.1588],\n",
      "        ...,\n",
      "        [-0.7643,  2.3739, -1.8800,  ..., -5.2282,  0.1946, -5.0154],\n",
      "        [-3.9107, -3.5252, -2.7594,  ..., -2.1235, -5.8703, -1.5282],\n",
      "        [-1.5923,  2.3207, -2.0654,  ..., -3.3131, -0.5611, -2.9758]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>) tensor(2.5604, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "print(logits, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda8644-ea37-41bd-994d-512aa84bc3f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8a791a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate method, to generate new tokens\n",
    "def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f381b",
   "metadata": {},
   "source": [
    "## Consolidation\n",
    "\n",
    "> Consolidating the above, my model now generates text outputs that are recognizably Shakespearean. The train loss is now 1.6488 and the val loss is 1.8093, which is a marked improvement.  In a future post I will work on certain aspects of the model to improve the performance further and confront it will different datasets.  The model's training and output is shown below. Please see __[my GitHub](https://github.com/johncollinsai)__ for the consolidated code, which I omit here because it slows the time it takes to open the post to an unacceptable level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc443ed-9e3e-4167-a0ed-9797f9513c38",
   "metadata": {},
   "source": [
    "0.209729 M parameters\n",
    "step 0: train loss 4.2614, val loss 4.2568 <br>\n",
    "step 100: train loss 2.6580, val loss 2.6605 <br>\n",
    "step 200: train loss 2.5000, val loss 2.5046 <br>\n",
    "step 300: train loss 2.4105, val loss 2.4309 <br>\n",
    "step 400: train loss 2.3485, val loss 2.3569 <br>\n",
    "step 500: train loss 2.2949, val loss 2.3087 <br>\n",
    "step 600: train loss 2.2367, val loss 2.2507 <br>\n",
    "step 700: train loss 2.1907, val loss 2.2175 <br>\n",
    "step 800: train loss 2.1549, val loss 2.1812 <br>\n",
    "step 900: train loss 2.1101, val loss 2.1578 <br>\n",
    "step 1000: train loss 2.0802, val loss 2.1154 <br>\n",
    "step 1100: train loss 2.0301, val loss 2.0998 <br>\n",
    "step 1200: train loss 2.0254, val loss 2.0861 <br>\n",
    "step 1300: train loss 1.9997, val loss 2.0595 <br>\n",
    "step 1400: train loss 1.9890, val loss 2.0445 <br>\n",
    "step 1500: train loss 1.9456, val loss 2.0219 <br>\n",
    "step 1600: train loss 1.9181, val loss 2.0008 <br>\n",
    "step 1700: train loss 1.9106, val loss 2.0062 <br>\n",
    "step 1800: train loss 1.8987, val loss 2.0026 <br>\n",
    "step 1900: train loss 1.8739, val loss 1.9658 <br>\n",
    "step 2000: train loss 1.8701, val loss 1.9788 <br>\n",
    "step 2100: train loss 1.8438, val loss 1.9617 <br>\n",
    "step 2200: train loss 1.8322, val loss 1.9344 <br>\n",
    "step 2300: train loss 1.8115, val loss 1.9326 <br>\n",
    "step 2400: train loss 1.8084, val loss 1.9267 <br>\n",
    "step 2500: train loss 1.7888, val loss 1.9249 <br>\n",
    "step 2600: train loss 1.7759, val loss 1.9167 <br>\n",
    "step 2700: train loss 1.7881, val loss 1.8967 <br>\n",
    "step 2800: train loss 1.7682, val loss 1.8924 <br>\n",
    "step 2900: train loss 1.7538, val loss 1.9109 <br>\n",
    "step 3000: train loss 1.7535, val loss 1.8929 <br>\n",
    "step 3100: train loss 1.7371, val loss 1.8828 <br>\n",
    "step 3200: train loss 1.7228, val loss 1.8752 <br>\n",
    "step 3300: train loss 1.7182, val loss 1.8677 <br>\n",
    "step 3400: train loss 1.7155, val loss 1.8733 <br>\n",
    "step 3500: train loss 1.7122, val loss 1.8637 <br>\n",
    "step 3600: train loss 1.7037, val loss 1.8632 <br>\n",
    "step 3700: train loss 1.6886, val loss 1.8564 <br>\n",
    "step 3800: train loss 1.6866, val loss 1.8321 <br>\n",
    "step 3900: train loss 1.6841, val loss 1.8379 <br>\n",
    "step 4000: train loss 1.6814, val loss 1.8447 <br>\n",
    "step 4100: train loss 1.6798, val loss 1.8399 <br>\n",
    "step 4200: train loss 1.6841, val loss 1.8392 <br>\n",
    "step 4300: train loss 1.6779, val loss 1.8295 <br>\n",
    "step 4400: train loss 1.6667, val loss 1.8330 <br>\n",
    "step 4500: train loss 1.6572, val loss 1.8032 <br>\n",
    "step 4600: train loss 1.6613, val loss 1.8300 <br>\n",
    "step 4700: train loss 1.6624, val loss 1.8185 <br>\n",
    "step 4800: train loss 1.6433, val loss 1.8098 <br>\n",
    "step 4900: train loss 1.6480, val loss 1.8206 <br>\n",
    "step 4999: train loss 1.6488, val loss 1.8093 <br>\n",
    "\n",
    "This price-nend; it wroable all more\n",
    "I the to be in much sruch on the chargen tell dent,\n",
    "Apurseticeit,--my regried, and mystory far Merch,\n",
    "Red city not we decemblemanvy wantwith a, a mirch\n",
    "Recors is rublence. You and AUo's faces fathee, the pausurals, and know\n",
    "Her no swoot he mest not of me?\n",
    "If rite and and true to latiul did crumb\n",
    "Though yout with little are in them,\n",
    "Frant? shall youble morry, thou march\n",
    "To have noble, bender will bit but\n",
    "In bisoved eyed\n",
    "She dick you.\n",
    "\n",
    "CORIOLANUS:\n",
    "So place to whence she were was me ence,\n",
    "So thou gamoust to a genaluest thee,\n",
    "Furst your not Lord sly, it, that\n",
    "Of but to brischarr with she peisul,\n",
    "What by you waves behis duke fife?\n",
    "\n",
    "First The foels.\n",
    "And that sit dest not to\n",
    "be work enters to cannowis cutle thrive with fother\n",
    "So firthed: do and I RI women with be orish with loved\n",
    "And to mothanking of out wut for nubly hast\n",
    "uphis farels didstruqer shee will.\n",
    "\n",
    "BUCHBOLANR:\n",
    "Duke you, to I she laking be got an keepost in thyse.\n",
    "\n",
    "GLOUCESTER:\n",
    "But, derrous not my denamous and.\n",
    "But and he duked from happy this furnily dears igk.\n",
    "\n",
    "Citid:\n",
    "The vorse disque tiruthlo.\n",
    "I with the life.\n",
    "\n",
    "Pirst That of, not the leaver off it wort?\n",
    "It I wovet the boundrangt, the pooly, Like a so!\n",
    "I how that crum the timet clring,\n",
    "Englink marry\n",
    "him a somet, not do?\n",
    "We sleep take jockita, with\n",
    "saward sweet in learent? throw thou know in knebears\n",
    "Twrue,\n",
    "Promeslesed hurse the brace, good's dear that?'\n",
    "\n",
    "Nurst known\n",
    "As belo.\n",
    "\n",
    "DUKE VINCENLOND:\n",
    "Heldsher mer my trine juke wuthers:\n",
    "We work, work. QUE:\n",
    "Where I would lossed a maber gived?\n",
    "\n",
    "Rethed Setchmness.\n",
    "\n",
    "PULY:\n",
    "O, wacrimb'd noble and live honesters,\n",
    "The giver to ut to go arminy theims?\n",
    "Call tis by reforge emaget,; dishes,\n",
    "Which sit dakes what she sayit\n",
    "As formoul muty cannot his hamelvenator:\n",
    "That goness, kneel. My comy,\n",
    "And say how a wife works Most, myce.\n",
    "Come, is that work, and me astshumble\n",
    "Formelf'd couses, yet should never frife!\n",
    "To well devoll on life, it no true,\n",
    "is marcher and shall spy give, not hother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea89cde2-ec75-4b03-8ea5-2542d4c8427d",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee8c82-06f7-4869-8671-eb941167b3b4",
   "metadata": {},
   "source": [
    "Brown, T.B., et al. (2020). Language Models are Few-Shot Learners. arXiv:2005.14165"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc668cf-bc7f-4d43-a6de-690a28cb9897",
   "metadata": {},
   "source": [
    "__[Colab for Kaparthy's video](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e95107",
   "metadata": {},
   "source": [
    "__[GitHub repo for Anton Bacaj's transformer architecture diagrams](https://github.com/abacaj/transformers)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7673317d-5bc1-4817-b349-1900a9410cc2",
   "metadata": {},
   "source": [
    "__[GitHub repo for Kaparthy's video](https://github.com/karpathy/ng-video-lecture)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7cab4-1542-4340-80ad-1989bcf689a2",
   "metadata": {},
   "source": [
    "__[Kaparthy's nanoGPT GitHub repo](https://github.com/karpathy/nanoGPT)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481fea7-f09f-409f-99fd-67c29b6a788c",
   "metadata": {},
   "source": [
    "__[Kaparthy's Youtube video](https://www.youtube.com/watch?v=kCc8FmEb1nY)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333d09d",
   "metadata": {},
   "source": [
    "Picard, D. (2021). Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision. arXiv:2109.08203 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99032892-1126-456b-8baf-424b0b00552a",
   "metadata": {},
   "source": [
    "Vaswani, A., et al. (2017).  Attention Is All You Need. arXiv:1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df5bcd-28f1-4d99-b7f8-f6685edc947c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-09T04:06:52.202330Z",
     "iopub.status.busy": "2023-02-09T04:06:52.202069Z",
     "iopub.status.idle": "2023-02-09T04:06:52.355032Z",
     "shell.execute_reply": "2023-02-09T04:06:52.354010Z",
     "shell.execute_reply.started": "2023-02-09T04:06:52.202312Z"
    }
   },
   "source": [
    "***\n",
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
